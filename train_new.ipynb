{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import load_data, get_loaders, get_probe_data, get_probe_loaders, make_dataset\n",
    "from transformer_predictor import TransformerPredictor, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data('Data/balanced_with_no_count.csv')\n",
    "# dataset = load_data('brackets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader, val_loader, test_loader, train_data, val_data, test_data = get_loaders(dataset, batch_size=BATCH_SIZE, return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 4000 12000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(val_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerPredictor(\n",
    "    input_dim=4,\n",
    "    model_dim=128,\n",
    "    num_classes=2,\n",
    "    num_heads=2,\n",
    "    num_layers=1,\n",
    "    lr=1e-3,\n",
    "    warmup=100,\n",
    "    max_iters=1000,\n",
    ")\n",
    "trainer = L.Trainer(max_epochs=10, devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res,model, trainer, train_outbeddings, val_outbeddings = train(model, trainer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res, train_outbeddings = test(model, trainer, train_loader)\n",
    "# res, val_outbeddings = test(model, trainer, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, test_outbeddings, _ = test(model, trainer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_outbeddings), len(val_outbeddings), len(test_outbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "no_count_df = pd.read_csv('new_test_data.csv')\n",
    "no_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all count values to absolute in no_count_df\n",
    "\n",
    "counts = no_count_df['count'].values\n",
    "no_count_df['count'] = [abs(int(count)) for count in counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_count = {}\n",
    "for count in no_count_df['count'].unique():\n",
    "    df_by_count[count] = no_count_df[no_count_df['count'] == count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_data_loading import TestBracketDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_count_df['count'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_by_count = {}\n",
    "results = []\n",
    "mlp_logits = []\n",
    "for count in no_count_df['count'].unique():\n",
    "    avg_attn_map = torch.zeros((2, 513, 513))\n",
    "    no_count_dataset = TestBracketDataset(df_by_count[count])\n",
    "    no_count_loader = DataLoader(no_count_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "    res, no_count_outbeddings, maps, logits = test(model, trainer, no_count_loader, attn=1)\n",
    "    results.append(res)\n",
    "    mlp_logits.append(logits)\n",
    "    for i in range(len(maps)):\n",
    "        for j in range(len(maps[i])):\n",
    "            avg_attn_map += maps[i][j]\n",
    "    avg_attn_map /= 200\n",
    "    maps_by_count[count] = avg_attn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_by_count[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save maps_by_count as a json\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "for count in maps_by_count:\n",
    "    maps_by_count_json = {}\n",
    "    maps_by_count_json[str(count)] = maps_by_count[count].detach().numpy().tolist()\n",
    "    with open(f'jsons/maps_by_count_{count}.json', 'w') as f:\n",
    "        json.dump(maps_by_count_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(len(results)):\n",
    "    acc.append(results[i][0]['test_acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies vs count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(no_count_df['count'].unique(), acc)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming `maps_by_count` is a dictionary containing attention maps for each count\n",
    "for count, attention_map in maps_by_count.items():\n",
    "    # Plot the attention map with sns heatmap\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    sns.heatmap(attention_map[1].detach().numpy())\n",
    "    plt.title(f'Attention map for count={count}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from data_loading import BracketDataset\n",
    "from main import Logger\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "def get_stack_depths(loader: DataLoader, stack_depths=[5, 15]):\n",
    "    # # stack_data = torch.where(torch.isin(data, torch.tensor(stack_depths)))[0]\n",
    "    # # get the indices of the data that have the stack depth in stack_depths\n",
    "    # print(len(data))\n",
    "    # stack_indices = [i for i, x in enumerate(data) if x['sd'] in stack_depths]\n",
    "\n",
    "    # # if stack_indices is empty, return None\n",
    "    # if len(stack_indices) == 0:\n",
    "    #     return None, 0\n",
    "    \n",
    "    # # create a new dataset with only the stack depths in stack_depths\n",
    "    # stack_data = torch.utils.data.Subset(data, stack_indices)\n",
    "    \n",
    "    # loader = DataLoader(stack_data, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # return loader, len(stack_indices)\n",
    "    stack_data = []\n",
    "    for batch in loader:\n",
    "        # print(batch)\n",
    "        for i, x in enumerate(batch['sd']):\n",
    "            # if type(x) is not dict:\n",
    "            #     print(x)\n",
    "            if x in stack_depths:\n",
    "                sample = {\n",
    "                    'x': batch['x'][i],\n",
    "                    'y': batch['y'][i],\n",
    "                    'sd': x,\n",
    "                    'eos': batch['eos'][i]\n",
    "                }\n",
    "                stack_data.append(sample)\n",
    "                                \n",
    "    if len(stack_data) == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    stack_dataset = BracketDataset(list=stack_data)\n",
    "    \n",
    "    stack_loader = DataLoader(stack_data, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return stack_loader, len(stack_data)\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(10, 101, 10):\n",
    "    stack_loader, len_stack = get_stack_depths(test_loader, stack_depths=[i])\n",
    "    if stack_loader is None:\n",
    "        continue\n",
    "    res, stack_outbeddings = test(model, trainer, stack_loader)\n",
    "    logger.log(f\"Stack depth {i}, Support: {len_stack}, Accuracy: {res}\")\n",
    "    # print(f\"Stack depth {i}, Support: {len_stack}, Accuracy: {res}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_probe_train, y_probe_train = get_probe_data(train_outbeddings, stack_depths=[15, 25])\n",
    "X_probe_val, y_probe_val = get_probe_data(val_outbeddings, stack_depths=[15, 25])\n",
    "X_probe_test, y_probe_test = get_probe_data(test_outbeddings, stack_depths=[15, 25])\n",
    "\n",
    "train_dataset = make_dataset(X_probe_train, y_probe_train)\n",
    "val_dataset = make_dataset(X_probe_val, y_probe_val)\n",
    "test_dataset = make_dataset(X_probe_test, y_probe_test)\n",
    "\n",
    "train_probe_loader, val_probe_loader, test_probe_loader = get_probe_loaders(train_dataset, val_dataset, test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe import Probe, probe_all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = Probe(model_name='lr')\n",
    "# probe.fit(train_probe_loader, val_probe_loader)\n",
    "# probe.probe(X_probe_train.cpu().detach().numpy(), y_probe_train.cpu().detach().numpy(), X_probe_test.cpu().detach().numpy(), y_probe_test.cpu().detach().numpy())\n",
    "probe_all_models(X_probe_train.cpu().detach().numpy(), y_probe_train.cpu().detach().numpy(), X_probe_test.cpu().detach().numpy(), y_probe_test.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.plot_decision_boundary(X_probe_test.cpu().detach().numpy(), y_probe_test.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
